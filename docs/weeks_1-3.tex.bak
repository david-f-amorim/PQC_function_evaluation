\documentclass{beamer}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[UKenglish]{babel}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{braket}
\usepackage{esint}
\usepackage{float}
\usepackage{subcaption}
\usepackage{hyperref}
\hypersetup{colorlinks=false, bookmarks=true}

\usetheme{Madrid}
\usecolortheme{seahorse}
\usefonttheme{professionalfonts}
\useinnertheme{circles}

\setbeamertemplate{caption}[numbered]

\title[PQC Function Evaluation]{PQC Function Evaluation}
\subtitle{Weeks 1-3}
\author[David Amorim]{David Amorim}
\institute[]{}
\date[22/07/2024]{01/07/2024 - 19/07/2024}

\begin{document}

\frame{\titlepage}

\begin{frame}
\frametitle{Table of Contents}
\tableofcontents
\end{frame}

\begin{frame}
\section{Background}
\frametitle{Background}

\begin{itemize}
\item Hayes 2023\footnote{\url{https://arxiv.org/pdf/2306.11073}} presents a scheme to encode a complex vector $\boldsymbol{h} =\{ \tilde{A}_j e^{i \Psi (j)} | 0 \leq j < N \}$ as the state 
\begin{equation}
\ket{h} = \frac{1}{|\tilde{A}|} \sum^{2^n-1}_{j=0} \tilde{A}(j) e^{i \Psi (j)} \ket{j}, 
\end{equation}
using $n = \lceil \log_2 N \rceil$ qubits
\item This requires operators $\hat{U}_A$ and $\hat{U}_\Psi$ such that 
\begin{align}
\hat{U}_A \ket{0}^{\otimes n} &=  \frac{1}{|\tilde{A}|} \sum^{2^n-1}_{j=0} \tilde{A}(j) \ket{j}, \\
\hat{U}_\Psi \ket{j} &= e^{i \Psi (j)} \ket{j}
\end{align}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Background}
\begin{itemize}
\item $\hat{U}_\Psi$ is constructed via an operator $\hat{Q}_\Psi$ that performs \alert{function evaluation} in an ancilla register:
\begin{equation}
\hat{Q}_\Psi  \ket{j} \ket{0}^{\otimes m}_a = \ket{j} \ket{\Psi'(j)}_a,
\end{equation}
with $\Psi'(j) \equiv \Psi(j) / 2 \pi$
\item Currently, $\hat{Q}_\Psi$ is implemented using gate-intensive \emph{linear piecewise functions (LPFs)}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Background}
\begin{alertblock}{Aim}
Implement $\hat{Q}_\Psi$ in a gate-efficient way using a parametrised quantum circuit (PQC)
\end{alertblock}
\vspace{1cm}
\textbf{Remark} \\
The $n$-qubit register containing the $\ket{j}$ and the $m$-qubit register containing the $\ket{\Psi'(j)}$ will be referred to as the \alert{input register} and \alert{target register}, respectively. 
\end{frame}

\begin{frame}
\section{Approach: a QCNN}
\frametitle{Approach: a QCNN}
\begin{itemize}
\item A \emph{quantum convolutional neural network} (\alert{QCNN}) is used to tackle the problem
\item A QCNN is a parametrised quantum circuit involving multiple \alert{layers}
\item Two types of network layers are implemented:
\begin{itemize}
\item \alert{Convolutional layers (CL)} involve multi-qubit entanglement gates 
\item \alert{Input layers (IL)}\footnote{Replacing the conventional QCNN \emph{pooling layers}} involve controlled single-qubit operations on target qubits 
\end{itemize}
\item Input qubits only appear as controls throughout the QCNN
\end{itemize}
\end{frame}
 
\begin{frame}
\subsection{Convolutional Layers}
\frametitle{Convolutional Layers (CLs)}
\begin{itemize}
\item Each CL involves the cascaded application of a \alert{two-qubit operator} on the target register 
\item A general two-qubit operator involves 15 parameters
\item To reduce the parameter space, the canonical \alert{three-parameter operator} 
\begin{equation}
\mathcal{N}(\alpha, \beta, \gamma) = \exp \left( i \left[ \alpha X \otimes X + \beta Y \otimes Y + \gamma Z \otimes Z \right] \right)
\end{equation}
is applied, at the cost of restricting the search space 
\item This can be decomposed\footnote{\url{https://arxiv.org/pdf/quant-ph/0308006}} into 3 $CX$, 3 $R_z$, and 2 $R_y$ gates
\item A two-parameter real version, $\mathcal{N}_\mathbb{R}(\lambda, \mu)$, can be obtained by removing the $R_z$
\end{itemize}
\end{frame} 

\begin{frame}
\frametitle{Convolutional Layers (CLs)}
\begin{itemize}
\item Two types of convolutional layers are implemented:
\begin{itemize}
\item \alert{Neighbour-to-neighbour / linear CLs}: the $\mathcal{N}$ (or $\mathcal{N}_\mathbb{R}$) gate is applied to neighbouring target qubits 
\item \alert{All-to-all /quadratic CLs}: the $\mathcal{N}$ (or $\mathcal{N}_\mathbb{R}$) gate is applied to all combinations of target qubits
\end{itemize}
\item The $\mathcal{N}$-gate cost of neighbour-to-neighbour (NN) layers is \alert{$\mathcal{O}(m)$} while that of all-to-all (AA) layers is \alert{$\mathcal{O}(m^2)$}
\item Currently, the QCNN uses alternating linear and quadratic CLs
\end{itemize}
\end{frame} 

\begin{frame}
\subsection{Input Layers}
\frametitle{Input Layers (ILs)}
\begin{itemize}
\item ILs, replacing pooling layers, feed information about the input register into the target register 
\item An IL involves a sequence of controlled generic single-qubit rotations (\alert{$CU3$ gates}) on the target qubits, with input qubits as controls
\item For an IL producing states with \alert{real} amplitudes, the $CU3$ gates are replaced with \alert{$CR_y$ gates}
\item Each input qubit controls precisely one $CU3$ (or $CR_y$ operation), resulting in an \alert{$\mathcal{O}(n)$} gate cost (no CX gates!)
\item ILs are inserted after every second convolutional layer, alternating between control states 0 and 1  
\end{itemize}
\end{frame}

\begin{frame}
\subsection{Summary: QCNN Structure}
\frametitle{Summary: QCNN Structure}
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{im/QCNN_structure}
\caption{Schematic of QCNN structure}
\end{figure}
\end{frame}

\begin{frame}
\section{Training the QCNN}
\frametitle{Training the QCNN}
\begin{itemize}
\item For training, the QCNN is wrapped as a \emph{SamplerQNN} object and connected to PyTorch's \alert{Adam optimiser} via \emph{TorchConnector}
\item The optimiser determines improved parameter values for each training run (\alert{epoch}) based on the \alert{loss} between output and target state
\item Beyond loss, \alert{mismatch} is an important metric:
\begin{equation}
\mathsf{M}= 1 - |\braket{\psi_\text{target}| \psi_\text{out}}|
\end{equation}
\item There are two ways to train the QCNN on input data:\footnote{One can also train the QCNN to produce a target distribution independent of the input register, which is equivalent to constructing $\hat{U}_A$}
\begin{enumerate}
\item Training on \alert{individual states}
\item Training in \alert{superposition}
\end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Training the QCNN}
\begin{block}{1. Training on Individual States}
\begin{itemize}
\item One of the $2^n$ input states, $\ket{j},$ is randomly chosen each epoch
\item The network is taught to transform $\ket{j}\ket{0} \mapsto \ket{j}\ket{\Psi'(j)} $ for each of the states individually
\end{itemize} 
\end{block}
%\vspace{1cm}
\begin{block}{2. Training in Superposition}
\begin{itemize}
\item The same input state is chosen each epoch
\item The network is taught to transform 
\begin{equation}
\left(\frac{1}{\sqrt{2^n}} \sum^{2^n-1}_{j=0} \ket{j} \right) \ket{0} \mapsto \frac{1}{\sqrt{2^n}} \sum^{2^n -1}_{j=0} \ket{j}\ket{\Psi'(j)}
\end{equation}
\item By linearity, this teaches the network to transform $\ket{j}\ket{0} \mapsto \ket{j}\ket{\Psi'(j)} $ for each $\ket{j}$
\end{itemize} 
\end{block}
\end{frame}

\begin{frame}
\section{Initial Tests}
\frametitle{Initial Tests}
\begin{itemize}
\item Initial tests need to be carried out to inform QCNN design choices regarding:
\begin{enumerate}[(a)]
\item Number of layers
\item Number of epochs 
\item Training mode (individually versus in superposition)
\item Use of $\mathcal{N}$ and $CU3$ versus $\mathcal{N}_\mathbb{R}$ and $R_y$ 
\item Choice of loss function
\item Network structure
\end{enumerate}
\item The case $n=m=2$, $\Psi(x)=x$ (the simplest non-trivial configuration) is an ideal \alert{benchmark problem}
\item Unclear, however, how well these findings extrapolate to more general cases
\end{itemize}
\end{frame}

\end{document}